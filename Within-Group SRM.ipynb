{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Within-Group SRM**\n",
    "\n",
    "- Build functions\n",
    "\t- Preprocess the raw data\n",
    "\t- Build a SRM model\n",
    "\t- Train a within-group SRM model\n",
    "\t- Calculate the ISC and permutation\n",
    "\t- Calculate permutation, find the thresholds of p-value and implement Autoaq command\n",
    "\n",
    "- Main process\n",
    "\t- Set selected features for each group\n",
    "\t- Get the pre-processed data\n",
    "\t- Train a SRM model and get reconstructed data\n",
    "\t- for two groups in high, mid and low\n",
    "\t\t- calculate the permutation of two groups\n",
    "\t\t- Find the thresholds(min corr difference of tow groups) for different p-values\n",
    "\t\t- Autoaq for each permutation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import brainiak\n",
    "import brainiak.funcalign.srm\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from brainiak import io, isc\n",
    "from nilearn import masking\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/data/neuro/LLS_audio/derivatives/analysis01/FUNC_reorg/'\n",
    "MASK_PATH = '/data/neuro/LLS_audio/derivatives/analysis01/FUNC_reorg/'\n",
    "\n",
    "SUBJ_NUM = 36\n",
    "MASK_NAME = 'EPI_{}_avg_mask.nii.gz'.format(SUBJ_NUM)\n",
    "\n",
    "LABELS = {\n",
    "    'high': [1, 2, 4, 8, 11, 16, 19, 20, 23, 28],\n",
    "    'mid': [3, 6, 7, 14, 21, 24, 32, 34, 35, 37],\n",
    "    'low': [5, 9, 10, 12, 13, 15, 17, 18, 22, 25, 26, 27, 30, 31, 33, 36]\n",
    "}\n",
    "LABEL_MAP = {'high': 0, 'mid': 1, 'low': 2}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-process all data from .nii.gz files\n",
    "\n",
    "\n",
    "class Processor:\n",
    "    def __init__(self):\n",
    "        self.preprocessed_data_fn = 'subj_run_masked.h5'\n",
    "        self.concatenated_data_fn = 'concatenated_masked.h5'\n",
    "        self.save_dir_p = 'data/preprocess'\n",
    "        self.preprocessed_data_p = os.path.join(self.save_dir_p, self.preprocessed_data_fn)\n",
    "        self.concatenated_data_p = os.path.join(self.save_dir_p, self.concatenated_data_fn)\n",
    "\n",
    "    def set_dir_path(self, p):\n",
    "        \"\"\"\n",
    "        Set directory path for saving files.\n",
    "        \n",
    "        :param p: str\n",
    "        \"\"\"\n",
    "        if not os.path.exists(p):\n",
    "            os.mkdir(p)\n",
    "\n",
    "        self.save_dir_p = p\n",
    "        self.preprocessed_data_p = os.path.join(self.save_dir_p, self.preprocessed_data_fn)\n",
    "        self.concatenated_data_p = os.path.join(self.save_dir_p, self.concatenated_data_fn)\n",
    "\n",
    "    def pre_process(self, rts=973):\n",
    "        \"\"\"\n",
    "        :param rts: \n",
    "        :return: \n",
    "        \"\"\"\n",
    "        if os.path.exists(self.concatenated_data_p):\n",
    "            return\n",
    "\n",
    "        mask_data = nib.load(os.path.join(MASK_PATH, MASK_NAME))\n",
    "        mask = np.array(mask_data.dataobj)\n",
    "\n",
    "        all_subj = []\n",
    "        all_subj_label = []\n",
    "\n",
    "        labels = LABELS\n",
    "        label_map = LABEL_MAP\n",
    "\n",
    "        for group_name, group in labels.items():\n",
    "            print(group_name, group)\n",
    "            for subj_i in sorted(group):\n",
    "                one_run = np.array([])\n",
    "                for run_i in [1, 2, 3, 4]:\n",
    "                    subj_fn = 'sub{:03}.run{:02}.func.resampl.nii.gz'.format(subj_i, run_i)\n",
    "                    subj_fp = os.path.join(DATA_PATH, subj_fn)\n",
    "\n",
    "                    if not os.path.exists(subj_fp):\n",
    "                        raise FileNotFoundError(subj_fp)\n",
    "\n",
    "                    voxels = masking.apply_mask(subj_fp, mask_data)  # shape=(tr, v)\n",
    "\n",
    "                    if len(one_run) == 0:\n",
    "                        one_run = voxels\n",
    "                    else:\n",
    "                        one_run = np.concatenate((one_run, voxels))\n",
    "\n",
    "                one_run = one_run.transpose()  # transposed to shape=(v, tr)\n",
    "\n",
    "                if one_run.shape != (np.sum(mask > 0), rts):\n",
    "                    raise ValueError('Shape is invalid: sub{:03} {}'.format(subj_i, one_run.shape))\n",
    "\n",
    "                print('... succeed sub{:03}'.format(subj_i))\n",
    "                all_subj.append(one_run)\n",
    "                all_subj_label.append(label_map[group_name])\n",
    "\n",
    "        fc = h5py.File(self.concatenated_data_p, 'w')\n",
    "        fc.create_dataset('all_runs', data=np.array(all_subj))\n",
    "        fc.create_dataset('labels', data=all_subj_label)\n",
    "        fc.close()\n",
    "\n",
    "\n",
    "processor = Processor()\n",
    "\n",
    "\n",
    "def pre_process(p):\n",
    "    print('Preprocessing...')\n",
    "    processor.set_dir_path(p)\n",
    "    processor.pre_process()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a SRM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a SRM model\n",
    "\n",
    "\n",
    "class SRMModel:\n",
    "    def __init__(self):\n",
    "        self.features = 50\n",
    "        self.n_iter = 20\n",
    "        self.model = None\n",
    "\n",
    "    def optimize_features_num(self):\n",
    "        return\n",
    "\n",
    "    def train_srm(self, train_data, features=50, n_iter=20):\n",
    "        \"\"\"\n",
    "\n",
    "        :param features:\n",
    "        :param n_iter:\n",
    "        :param train_data: shape = (m, v, tr)\n",
    "        \"\"\"\n",
    "        self.features = features\n",
    "        self.n_iter = n_iter\n",
    "        self.model = brainiak.funcalign.srm.SRM(features=self.features, n_iter=self.n_iter)\n",
    "\n",
    "        for subject in range(len(train_data)):\n",
    "            train_data[subject] = stats.zscore(train_data[subject], axis=1, ddof=1)\n",
    "            train_data[subject] = np.nan_to_num(train_data[subject])\n",
    "\n",
    "        self.model.fit(train_data)\n",
    "\n",
    "    def reconstruct(self, data):\n",
    "        \"\"\"\n",
    "\n",
    "        :param data: shape = (m, v, tr)\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        m, v, tr = data.shape\n",
    "\n",
    "        # Transform the data into the shared space using the individual weight matrices\n",
    "        shared = self.model.transform(data)  # shape = (m ,f, tr)\n",
    "\n",
    "        # Zscore the transformed data\n",
    "        for subject in range(m):\n",
    "            shared[subject] = stats.zscore(shared[subject], axis=1, ddof=1)\n",
    "            shared[subject] = np.nan_to_num(shared[subject])\n",
    "\n",
    "        # Do the reconstruction on all individual participants and organize it for ISC\n",
    "\n",
    "        signal_srm = np.zeros((m, v, tr))\n",
    "\n",
    "        for subject in range(m):\n",
    "            signal_srm[subject, :, :] = self.model.w_[subject].dot(shared[subject])\n",
    "            signal_srm[subject] = np.nan_to_num(signal_srm[subject])\n",
    "\n",
    "        return signal_srm\n",
    "\n",
    "\n",
    "srm_model = SRMModel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a within-group SRM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a SRM model within each group\n",
    "\n",
    "\n",
    "def train_srm_model(by, **kwargs):\n",
    "    \"\"\"\n",
    "    Train a SRM model.\n",
    "    :param by: str 'group' for within-group SRM or 'all for across-group SRM\n",
    "    :param kwargs: \n",
    "    \"\"\"\n",
    "    if 'save_dir_p' in kwargs.keys():\n",
    "        if not os.path.exists(kwargs['save_dir_p']):\n",
    "            os.mkdir(kwargs['save_dir_p'])\n",
    "    \n",
    "    if by == 'group':\n",
    "        train_srm_model_by_group(**kwargs)\n",
    "\n",
    "\n",
    "def train_srm_model_by_group(features, save_dir_p=None):\n",
    "    if not save_dir_p:\n",
    "        save_dir_p = processor.save_dir_p\n",
    "\n",
    "    if os.path.exists(os.path.join(save_dir_p, 'group_data.h5')):\n",
    "        return\n",
    "\n",
    "    print('Training SRM...')\n",
    "    f = h5py.File(processor.concatenated_data_p, 'r')\n",
    "    all_runs = f['all_runs'][:]\n",
    "    all_runs_labels = f['labels'][:]\n",
    "\n",
    "    f = h5py.File(os.path.join(save_dir_p, 'group_data.h5'), 'w')\n",
    "    f.close()\n",
    "\n",
    "    f = h5py.File(os.path.join(save_dir_p, 'group_data.h5'), 'r+')\n",
    "    print('Saving reconstructed data...')\n",
    "    for group in ['high', 'mid', 'low']:\n",
    "        group_runs = all_runs[all_runs_labels == LABEL_MAP[group]]\n",
    "\n",
    "        if isinstance(features, int):\n",
    "            srm_model.train_srm(group_runs, features=features)\n",
    "        elif isinstance(features, str):\n",
    "            f_list = features.split('_')\n",
    "            selected_f = int(f_list[LABEL_MAP[group]])\n",
    "            srm_model.train_srm(group_runs, features=selected_f)\n",
    "\n",
    "        group_runs = srm_model.reconstruct(group_runs)\n",
    "        f.create_dataset(group, data=group_runs)\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the ISC and permutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the ISC and permutation for each group\n",
    "\n",
    "\n",
    "def calculate_isc(data, **kwargs):\n",
    "    corr = isc.isc(data, **kwargs)\n",
    "    corr = np.nan_to_num(corr)\n",
    "    return corr\n",
    "\n",
    "\n",
    "def calculate_permutation_isc(all_subj_corr, labels=None, **kwargs):\n",
    "    \"\"\"\n",
    "    Note: either one group or two group in your labels\n",
    "\n",
    "    :param all_subj_corr:\n",
    "    :param labels:\n",
    "    :param kwargs:\n",
    "    \"\"\"\n",
    "    n_permutations = 1000\n",
    "    summary_statistic = 'mean'\n",
    "\n",
    "    if 'summary_statistic' in kwargs.keys():\n",
    "        summary_statistic = kwargs['summary_statistic']\n",
    "    if 'n_permutations' in kwargs.keys():\n",
    "        n_permutations = kwargs['n_permutations']\n",
    "\n",
    "    observed, p, distribution = isc.permutation_isc(\n",
    "        all_subj_corr,\n",
    "        pairwise=False,\n",
    "        group_assignment=labels,\n",
    "        summary_statistic=summary_statistic,\n",
    "        n_permutations=n_permutations,\n",
    "        **kwargs\n",
    "    )\n",
    "    return observed, p, distribution\n",
    "\n",
    "\n",
    "def calculate_iscs(group, summary_statistic=None, save_dir_p=None):\n",
    "    if not save_dir_p:\n",
    "        save_dir_p = 'data/preprocess'\n",
    "\n",
    "    fn = '{}reconst_isc.h5'.format((summary_statistic + '_') if summary_statistic else '')\n",
    "    fp = os.path.join(save_dir_p, fn)\n",
    "\n",
    "    if not os.path.exists(fp):\n",
    "        print('Create new .h5 file at', fp)\n",
    "        read_type = 'w'\n",
    "    else:\n",
    "        print('Read existed .h5 file', fp)\n",
    "        read_type = 'r+'\n",
    "    f_c = h5py.File(fp, read_type)\n",
    "\n",
    "    if group in f_c.keys():\n",
    "        return\n",
    "\n",
    "    print('Reading reconstructed data...')\n",
    "    f_rg = h5py.File(os.path.join(save_dir_p, 'group_data.h5'), 'r')\n",
    "    group_data = f_rg[group][:]\n",
    "    f_rg.close()\n",
    "\n",
    "    print('Calculating ISC...')\n",
    "    group_corr = calculate_isc(group_data.transpose(), summary_statistic=summary_statistic)\n",
    "\n",
    "    print('Saving ISC...')\n",
    "\n",
    "    f_c.create_dataset(group, data=group_corr)\n",
    "    f_c.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate permutation, find the thresholds of p-value and implement Autoaq command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_group_permutation(ftype, save_dir_p, is_save=True):\n",
    "    \"\"\"\n",
    "    Calculate the permutation of two groups.\n",
    "    \n",
    "    :param ftype: ['hl', 'hm', 'ml', 'lh', 'mh', 'lm']\n",
    "    :param save_dir_p: \n",
    "    :param is_save: \n",
    "    \"\"\"\n",
    "    for group in ['low', 'mid', 'high']:\n",
    "        print('\\nGroup', group)\n",
    "        calculate_iscs(group, save_dir_p=save_dir_p)\n",
    "\n",
    "    print('Reading reconstructed isc...')\n",
    "    fp = os.path.join(save_dir_p, 'reconst_isc.h5')\n",
    "    f_c = h5py.File(fp, 'r')\n",
    "\n",
    "    grp_name = {\n",
    "        'h': 'high',\n",
    "        'l': 'low',\n",
    "        'm': 'mid'\n",
    "    }\n",
    "    corr_type_0, corr_type_1 = ftype[0], ftype[1]\n",
    "    corr_0 = f_c[grp_name[corr_type_0]][:]\n",
    "    corr_1 = f_c[grp_name[corr_type_1]][:]\n",
    "    f_c.close()\n",
    "\n",
    "    _corr = np.concatenate((corr_0, corr_1), axis=0)\n",
    "    _labels = [0] * len(corr_0) + [1] * len(corr_1)\n",
    "    observed, p, distribution = calculate_permutation_isc(_corr, _labels)\n",
    "    if is_save:\n",
    "        with h5py.File('output/{}_permutation.h5'.format(ftype), 'w') as f:\n",
    "            f.create_dataset('observed', data=observed)\n",
    "            f.create_dataset('p', data=p)\n",
    "            f.create_dataset('distribution', data=distribution)\n",
    "            f.close()\n",
    "\n",
    "\n",
    "def autoaq_2grp(ftype):\n",
    "    \"\"\"\n",
    "    Autoaq command line.\n",
    "    \n",
    "    :param ftype: ['hl', 'hm', 'ml', 'lh', 'mh', 'lm']\n",
    "    \"\"\"\n",
    "\n",
    "    def get_isc_image(mask_p, corr, img_name=None, is_save=True):\n",
    "        print('Writing ISC map to file...')\n",
    "\n",
    "        # Map the ISC data for the participant into brain space\n",
    "        brain_template = nib.load(mask_p)\n",
    "        mask_image = io.load_boolean_mask(mask_p)\n",
    "        coords = np.where(mask_image)\n",
    "        isc_vol = np.zeros(brain_template.shape)\n",
    "        isc_vol[coords] = corr\n",
    "\n",
    "        # make a nii image of the isc map\n",
    "        isc_image = nib.Nifti1Image(isc_vol, brain_template.affine, brain_template.header)\n",
    "\n",
    "        if is_save:\n",
    "            nib.save(isc_image, 'output/{}_isc.nii.gz'.format(img_name))\n",
    "\n",
    "    print('Reading permutation result...')\n",
    "    fp = os.path.join('output', '{}_permutation.h5'.format(ftype))\n",
    "    f_c = h5py.File(fp, 'r')\n",
    "\n",
    "    mask_path = os.path.join(MASK_PATH, MASK_NAME)\n",
    "\n",
    "    print('Saving image...')\n",
    "    observed = f_c['observed'][:]\n",
    "    get_isc_image(mask_p=mask_path, corr=observed, img_name='observed_{}'.format(ftype))\n",
    "\n",
    "    print('Autoaq...')\n",
    "    observed_fp = os.path.join('output', 'observed_{}_isc.nii.gz'.format(ftype))\n",
    "    result_fp = os.path.join('output', 'subj36_srm_isc_{}_Talairach.txt'.format(ftype))\n",
    "    os.system('autoaq -i {} -a \"Talairach Daemon Labels\" '\n",
    "              '-t 0.1 -u -p -o {}'.format(observed_fp, result_fp))\n",
    "\n",
    "\n",
    "def analysis_permutation_2grp(ftype):\n",
    "    \"\"\"\n",
    "    Get the threshold of ISC difference.\n",
    "    \n",
    "    :param ftype: ['hl', 'hm', 'ml', 'lh', 'mh', 'lm']\n",
    "    \"\"\"\n",
    "    print('Reading permutation result...')\n",
    "    fp = 'output/{}_permutation.h5'.format(ftype)\n",
    "\n",
    "    f = h5py.File(fp, 'r')\n",
    "    p_value = f['p'][:]\n",
    "    observed = f['observed'][:]\n",
    "\n",
    "    def write_log(p, text):\n",
    "        with open(os.path.join(p, 'permutation_threshold.txt'), 'a') as _f:\n",
    "            _f.write(text)\n",
    "\n",
    "    print('Find threshold of ftype:', ftype)\n",
    "    for sig in [0.05, 0.01, 0.005,.001]:\n",
    "        threshold = np.min(np.abs(observed[np.where(p_value < sig)]))\n",
    "        content = '{} threshold of ISC difference isÂ {:.2f}, p<{}'.format(ftype, threshold, sig)\n",
    "        print(content)\n",
    "        write_log('output', content + '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Main process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set selected features for different groups\n",
    "s_feature = '40_55_30'  # high, mid, low\n",
    "train_type = 'group'\n",
    "save_path = 'data/preprocess/subj_{}_feature_{}/by_{}'.format(s_feature, SUBJ_NUM, train_type)\n",
    "\n",
    "# Pre-process the raw data\n",
    "pre_process(save_path)\n",
    "\n",
    "# Train a within-group SRM model\n",
    "train_type = 'group'\n",
    "train_srm_model(by=train_type, save_dir_p=save_path, features=s_feature)\n",
    "\n",
    "for ft in ['hl', 'hm', 'ml', 'lh', 'mh', 'lm']:\n",
    "    # Calculate the permutation of two groups\n",
    "    get_two_group_permutation(ft, save_path)\n",
    "\n",
    "    # Find the thresholds(min corr difference of tow groups) for different p-values\n",
    "    analysis_permutation_2grp(ft)\n",
    "\n",
    "    # Autoaq bash command\n",
    "    autoaq_2grp(ft)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
