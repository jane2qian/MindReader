{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-step SRM\n",
    " @Ruiqing Zhang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ ### Generate GM mask using all sub's highres.nii.gz \n",
    "\n",
    "+ ### Concatnate all runs, generate train dataset  \n",
    "\n",
    "+ ### Time segment matching\n",
    "\n",
    "+ ### First step SRM for all subjects\n",
    "\n",
    "+ ### Second step SRM within group\n",
    "\n",
    "+ ### ISC and permutation test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import abspath, dirname, join\n",
    "from brainiak import image, io\n",
    "from brainiak.isc import isc, isfc, compute_summary_statistic,permutation_isc\n",
    "from brainiak.fcma.util import compute_correlation\n",
    "import brainiak.funcalign.srm\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import h5py\n",
    "from itertools import combinations\n",
    "from scipy.cluster.hierarchy import fcluster, linkage, dendrogram\n",
    "from sklearn.cluster import KMeans\n",
    "import scipy.spatial.distance as sp_distance\n",
    "from sklearn.manifold import TSNE\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, pearsonr, zscore\n",
    "from scipy.spatial.distance import squareform\n",
    "from statsmodels.sandbox.stats.multicomp import multipletests\n",
    "from nilearn import datasets,image,masking\n",
    "from nilearn.input_data import NiftiMasker, NiftiLabelsMasker\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Subjects info table\n",
    " \n",
    " |Proficiency|  N   |ACC_Comp | subID |\n",
    " |:---:|: --- |: ---| :--- |\n",
    " |HPL|10|0.861538462| 1,2,4,8,11,16,19,20,23,28|\n",
    " |MPL|9|0.671328671|3,6,7,14,21,24,32,35,37|  \n",
    " |LPL|16|0.415865385|5,9,10,12,13,15,17,18,22,25,26,27,30,31,33,34,36|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir=dirname('/data/neuro/LLS_audio/')\n",
    "fdir = dirname(join(root_dir,'derivatives/analysis01/FUNC_reorg/'))\n",
    "PL=[[1,2,4,8,11,16,19,20,23,28],[3,6,7,14,21,24,32,34,35,37],[5,9,10,12,13,15,17,18,22,25,26,27,30,31,33,36]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- #### organize train data for SRM\n",
    "\n",
    "   subejcts= (range(1,29),range(30,38)) \n",
    "   \n",
    "   sub029 is removed due to the earphone problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_train_dset(mask,subjects):\n",
    "        f = h5py.File('train_dset.h5','w')\n",
    "        mask=nib.load(join(fdir,mask))\n",
    "        train_dset = []\n",
    "        test_dset= []\n",
    "        masked_img = []\n",
    "        images_concatenated=[]\n",
    "        \n",
    "        for subject in subjects:\n",
    "                for sub in subject:\n",
    "                        dset=[]\n",
    "                        dset += [join(fdir, 'sub{:03d}.run{:02d}.func.resampl.nii.gz').format(sub,run)  for run in range(1,5)]\n",
    "                        masked_img = masking.apply_mask(dset,mask)\n",
    "                        data = np.array(masked_img)\n",
    "                        images_concatenated.append(masked_img)\n",
    "        \n",
    "        data=np.array(images_concatenated)    \n",
    "        train_dset=np.swapaxes(data,1,2)\n",
    "        num_subs, vox_num, nTR = train_dset.shape\n",
    "        print('train dataset: ',\n",
    "                'Participants ', num_subs,\n",
    "                'Voxels per participant ', vox_num,\n",
    "                'TRs per participant ', nTR)\n",
    "        for sub in range(len(train_dset)):\n",
    "                train_dset[sub] = stats.zscore(train_dset[sub],axis=1,ddof=1)\n",
    "                train_dset[sub] = np.nan_to_num(train_dset[sub]) \n",
    "        \n",
    "        f.create_dataset(('train_dset_{}').format(num_subs),data=train_dset)\n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ #### validation performance of the feature in second step SRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_segment_matching(data, win_size=10): \n",
    "    nsubjs = len(data)\n",
    "    (ndim, nsample) = data[0].shape\n",
    "    accu = np.zeros(shape=nsubjs)\n",
    "    nseg = nsample - win_size \n",
    "    \n",
    "    # mysseg prediction\n",
    "    trn_data = np.zeros((ndim*win_size, nseg),order='f')\n",
    "    \n",
    "    # the training data also include the test data, but will be subtracted when calculating A\n",
    "    for m in range(nsubjs):\n",
    "        for w in range(win_size):\n",
    "            trn_data[w*ndim:(w+1)*ndim,:] += data[m][:,w:(w+nseg)]\n",
    "    for tst_subj in range(nsubjs):\n",
    "        tst_data = np.zeros((ndim*win_size, nseg),order='f')\n",
    "        for w in range(win_size):\n",
    "            tst_data[w*ndim:(w+1)*ndim,:] = data[tst_subj][:,w:(w+nseg)]\n",
    "\n",
    "        A =  np.nan_to_num(stats.zscore((trn_data - tst_data),axis=0, ddof=1))\n",
    "        B =  np.nan_to_num(stats.zscore(tst_data,axis=0, ddof=1))\n",
    "\n",
    "        # compute correlation matrix\n",
    "        corr_mtx = compute_correlation(B.T,A.T)\n",
    "\n",
    "        # The correlation classifier.\n",
    "        for i in range(nseg):\n",
    "            for j in range(nseg):\n",
    "                # exclude segments overlapping with the testing segment\n",
    "                if abs(i-j)<win_size and i != j :\n",
    "                    corr_mtx[i,j] = -np.inf\n",
    "        max_idx =  np.argmax(corr_mtx, axis=1)\n",
    "        accu[tst_subj] = sum(max_idx == range(nseg)) / nseg\n",
    "\n",
    "        # Print accuracy\n",
    "        print(\"Accuracy for subj %d is: %0.4f\" % (tst_subj, accu[tst_subj] ))\n",
    "        \n",
    "    print(\"The average accuracy among all subjects is {0:f} +/- {1:f}\".format(np.mean(accu), np.std(accu)))\n",
    "    return accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ #### First step SRM to get residuals by substracting recons_dset from train_dset\n",
    "    \n",
    "    dset='train_dset_{}'.format(num_subs)\n",
    "    \n",
    "    params = [10,20]\n",
    "    n_iter = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_step_srm(params,n_iter,dset):\n",
    "    f = h5py.File('train_dset.h5','r')\n",
    "    for k1 in params:\n",
    "        train_dset = f[dset][:]\n",
    "        print('feature numbers :', k1, 'srm iterations : ', n_iter)\n",
    "        \n",
    "        srm = brainiak.funcalign.srm.SRM(n_iter=n_iter, features=k1,)\n",
    "        srm.fit(train_dset)\n",
    "        print('SRM has been fit')\n",
    "        \n",
    "        shared_train = srm.transform(train_dset)\n",
    "        for sub in range(len(shared_train)):\n",
    "                shared_train[sub] = stats.zscore(shared_train[sub],axis=1,ddof=1)\n",
    "                shared_train[sub] = np.nan_to_num(shared_train[sub])\n",
    "        \n",
    "        f1 = h5py.File('shared_signal_{}_{}.h5'.format(k1,n_iter),'w')\n",
    "        f1.create_dataset(('shared_signal_s1_{}_{}').format(k1,n_iter),data=shared_train)\n",
    "        print('shared train data of srm1 is saved!')\n",
    "        \n",
    "        recons_dset = np.zeros((train_dset.shape))\n",
    "        for p in range(len(recons_dset)):\n",
    "                w=srm.w_[p]\n",
    "                recons_dset[p,:,:]=w.dot(shared_train[p])\n",
    "                recons_dset[p] = stats.zscore(recons_dset[p])\n",
    "                recons_dset[p] = np.nan_to_num(recons_dset[p])\n",
    "        \n",
    "        f2 = h5py.File('recons_signal_{}_{}.h5'.format(k1,n_iter),'w')\n",
    "        f2.create_dataset(('recons_signal_s1_{}_{}').format(k1,n_iter),data=recons_dset)\n",
    "        print('reconstruction data of srm1 is saved!')\n",
    "       \n",
    "        residuals = np.zeros((train_dset.shape))\n",
    "        for s in range(len(residuals)):\n",
    "            residuals[s]=np.subtract(train_dset[s],recons_dset[s])\n",
    "        \n",
    "        f3 = h5py.File('residuals_{}_{}.h5'.format(k1,n_iter),'w')\n",
    "        f3.create_dataset(('residuals_{}_{}').format(k1,n_iter),data=residuals)\n",
    "        \n",
    "        f1.close();f2.close();f3.close()\n",
    "        del train_dset\n",
    "        del shared_train\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+  #### Voxel Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def voxel_selection(k1,n_iter):\n",
    "    residuals_name = ('residuals_{}_{}').format(k1,n_iter)\n",
    "    f = h5py.File('residuals_{}_{}.h5'.format(k1,n_iter),'r')\n",
    "    dset = f[residuals_name][:] \n",
    "    data  = np.zeros((dset[0].shape[1],dset[0].shape[0],len(dset))) \n",
    "    for i in range(len(dset)):\n",
    "        data[:,:,i] = dset[i].T\n",
    "    print('ISC computation begins...')\n",
    "    iscs = isc(data,summary_statistic='mean',tolerate_nans=0.85)\n",
    "    iscs = np.nan_to_num(iscs)\n",
    "    print('permutation test begins...')\n",
    "    observed, p, distribution = bootstrap_isc(iscs)\n",
    "    if  np.where(p<0.001) != np.empty:\n",
    "        sig = 0.001\n",
    "        ind = np.where(p<0.001)\n",
    "        print('permutation test in residuals: found significant voxels in p<0.001') \n",
    "    elif  np.where(p<0.005) != np.empty:\n",
    "        sig = 0.005\n",
    "        ind = np.where(p<0.005)\n",
    "        print('permutation test in residuals: found significant voxels in p<0.005')\n",
    "    elif np.where(p<0.05) == np.empty:\n",
    "        sig = 0.05\n",
    "        ind = np.where(p<0.05)\n",
    "        print('permutation test in residuals: found no statistical significant voxels in p < 0.05')\n",
    "    observed[ind]=1\n",
    "    observed[~ind]=0  \n",
    "    coords = np.where(observed)\n",
    "    masked_residual = np.zeros(len(dset),len(ind),dset[0].shape[1])\n",
    "    for sub in range(len(dset)):\n",
    "        masked_residual[sub] = dset[sub][coords,:]\n",
    "    print('masked_residuals shape is :',masked_residual.shape)\n",
    "    ff = h5py.File('masked_residual_k1{}_n_iter{}_sig{}.h5'.format(k1,n_iter,sig),'w')\n",
    "    ff.create_dataset('masked_residual_k1{}_n_iter{}_sig{}'.format(k1,n_iter,sig),data=masked_residual)\n",
    "    ff.close()\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ #### Second step SRM to fit residuals within-group \n",
    " \n",
    "    K1 = [20,50] params = [50,100] n_iter =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def second_step_srm(k1,params,n_iter):\n",
    "        '''\n",
    "        group srm and validation \n",
    "        '''\n",
    "        print('second srm fit within-group data')\n",
    "        residuals_name = ('residuals_{}_{}').format(k1,n_iter)\n",
    "        f = h5py.File('residuals_{}_{}.h5'.format(k1,n_iter),'r')\n",
    "        residuals = f[residuals_name][:]\n",
    "        \n",
    "        for i in PL:\n",
    "                train_dset2 = np.zeros((len(i),residuals[0].shape[0],residuals[0].shape[1]))\n",
    "                for j in range(len(i)):\n",
    "                        id = i[j]-1\n",
    "                        if i[j] < 30:\n",
    "                                print('extracting subject',id+1)\n",
    "                                train_dset2[j]=residuals[id]\n",
    "                        else:\n",
    "                                print('extracting subject',id+1) \n",
    "                                train_dset2[j]=residuals[id-1]\n",
    "                num_subs,n_voxels,nTR = train_dset2.shape\n",
    "                print('subjects number in 2ed srm:',num_subs,\n",
    "                        '\\n number of voxels:',n_voxels ,\n",
    "                        '\\n number of TRs:',nTR)\n",
    "                for k2 in params:\n",
    "                        srm2 =brainiak.funcalign.srm.SRM(n_iter=n_iter, features=k2,) \n",
    "                \n",
    "                print('second step srm fitting within-group data...', \n",
    "                        '\\n features number ', k2 , '\\n iteration', n_iter)\n",
    "                \n",
    "                srm2.fit(train_dset2)\n",
    "                shared_train2 = srm2.transform(train_dset2)\n",
    "              \n",
    "                for sub in range(len(shared_train2)):\n",
    "                        shared_train2[sub] = stats.zscore(shared_train2[sub],axis=1,ddof=1)\n",
    "                        shared_train2[sub] = np.nan_to_num(shared_train2[sub])\n",
    "               \n",
    "                recons_dset2=np.zeros((train_dset2.shape))\n",
    "                for p in range(len(train_dset2)):\n",
    "                        w=srm2.w_[p]\n",
    "                        recons_dset2[p]=w.dot(shared_train2[p])\n",
    "                        recons_dset2[p] = stats.zscore(recons_dset2[p])\n",
    "                        recons_dset2[p] = np.nan_to_num(recons_dset2[p])\n",
    "                \n",
    "           \n",
    "                groups=dict([('H',PL[0]),('M',PL[1]),('L',PL[2])])\n",
    "                for key, value in groups.items():\n",
    "                        if value == i:\n",
    "                                f1 = h5py.File(('shared_'+ key +'G_k1_{}_k2_{}.h5').format(k1,k2),'w')\n",
    "                                f2 = h5py.File(('recons_'+ key +'G_k1_{}_k2_{:}.h5').format(k1,k2),'w')\n",
    "                                f1.create_dataset(('shared_'+ key +'G_k1_{}_k2_{}').format(k1,k2),data=shared_train2)\n",
    "                               \n",
    "                                f2.create_dataset(('recons_'+ key +'G_k1_{}_k2_{:}').format(k1,k2),data=recons_dset2)\n",
    "                                            \n",
    "                print('validation parameters :',\n",
    "                        '\\n subjects ID:', i,\n",
    "                        '\\n k:', k2)\n",
    "                #accuracy = time_segment_matching(recons_dset2,win_size=10)\n",
    "                #np.savetxt(ot_name,accuracy)\n",
    "                f1.close();f2.close()\n",
    "        \n",
    "        f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ #### Validation using TSM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(group,k1,k2):\n",
    "    f=h5py.File('recons_{}G_k1_{}_k2_{}.h5'.format(group,k1,k2),'r')\n",
    "    dset=f['recons_{}G_k1_{}_k2_{}'.format(group,k1,k2)][:]\n",
    "    accuracy = time_segment_matching(dset,win_size=10)\n",
    "    ot_name=join(fdir,'ts_acc_{}G_k1_{}_k2_{}.txt').format(group,k1,k2) \n",
    "    np.savetxt(ot_name,accuracy)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ #### permutation test for group ISC \n",
    "\n",
    "   group is two samples : ['H','L'] or just one sample : ['H']\n",
    "   \n",
    "   k1=[20,50] k2=[50,100]\n",
    "   \n",
    "   mask = 'GM_{}_mask.nii.gz'.format(num_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def permut_isc(group,k1,k2,mask):\n",
    "    GM_epi_mask = join(fdir,'results',mask)\n",
    "    bi_mask=io.load_boolean_mask(GM_epi_mask, lambda x: x>0.5)\n",
    "    coords =np.where(bi_mask)\n",
    "    dset = []\n",
    "    groups=dict([('H',len(PL[0])),('M',len(PL[1])),('L',len(PL[2]))])\n",
    "    \n",
    "    if len(group)==1:\n",
    "        f1 = h5py.File('permutation_isc_p_{}_{}.h5'.format(k1,k2),'w')\n",
    "        f2 = h5py.File('permutation_isc_value_{}_{}.h5'.format(k1,k2),'w')\n",
    "        f = h5py.File('recons_{}G_k1_{}_k2_{}'.format(group[0],k1,k2),'r')\n",
    "        dset = f[('recons_{}G_k1_{}_k2_{}'.format(group[0],k1,k2))][:]\n",
    "        label = None\n",
    "        p_name = ('{}_p_k1_{}_k2_{}'.format(group[0],k1,k2))\n",
    "        isc_name = ('{}_observed_{}_k2_{}.nii.gz'.format(group[0],k1,k2)) \n",
    "    \n",
    "    elif len(group)==2:\n",
    "        f1 = h5py.File('permutation_isc_p_{}vs{}_{}_{}.h5'.format(group[0],group[1],k1,k2),'w')\n",
    "        f2 = h5py.File('permutation_isc_value_{}vs{}_{}_{}.h5'.format(group[0],group[1],k1,k2),'w')\n",
    "        f = h5py.File('recons_{}G_k1_{}_k2_{}'.format(group[0],k1,k2),'r')\n",
    "        ff = h5py.File('recons_{}G_k1_{}_k2_{}'.format(group[1],k1,k2),'r')\n",
    "        dset = np.concatenate((f['recons_{}G_k1_{}_k2_{}'.format(group[0],k1,k2)][:],ff['recons_{}G_k1_{}_k2_{}'.format(group[1],k1,k2)][:]))\n",
    "        label = np.concatenate((np.zeros(groups.get('{}'.format(group[0]))),np.ones(groups.get('{}'.format(group[1]))))) \n",
    "        p_name = ('diff_{}vs{}_p_k1_{}_k2_{}'.format(group[0],group[1],k1,k2))\n",
    "        isc_name = ('diff_{}vs{}_observed_k1_{}_k2_{}.nii.gz'.format(group[0],group[1],k1,k2)) \n",
    "\n",
    "    f.close()\n",
    "    data  = np.zeros((dset[0].shape[1],dset[0].shape[0],len(dset))) \n",
    "    for i in range(len(dset)):\n",
    "        data[:,:,i] = dset[i].T\n",
    "    \n",
    "    iscs = isc(data,summary_statistic='mean',tolerate_nans=0.85)\n",
    "    iscs = np.nan_to_num(iscs)\n",
    "    observed, p, distribution = permutation_isc(iscs,group_assignment=label) \n",
    "    \n",
    "    if  np.where(p<0.001) != np.empty:\n",
    "        sig = 0.001\n",
    "        print('permutation test in group {}: found significant voxels in p<0.001'.format(group)) \n",
    "    elif  np.where(p<0.005) != np.empty:\n",
    "        sig = 0.005\n",
    "        print('permutation test in group {}: found significant voxels in p<0.005'.format(group))\n",
    "    elif np.where(p<0.05) == np.empty:\n",
    "        sig = 0.5\n",
    "        print('permutation test in group {}: found no statistical significant voxels in p < {}'.format(group,sig))\n",
    "    \n",
    "    ind = np.where(p<sig)\n",
    "    f1.create_dataset(p_name,data=p)\n",
    "    f2.create_dataset(isc_name[:-7],data=observed)\n",
    "    f.close();f1.close();f2.close()\n",
    "    nii_template = nib.load(GM_epi_mask)\n",
    "    out_isc = np.zeros(nii_template.shape)\n",
    "    out_isc[coords] = observed\n",
    "    isc_obj = nib.Nifti1Image(out_isc,nii_template.affine,nii_template.header)\n",
    "    nib.save(isc_obj,isc_name)\n",
    "    threshod = min(np.abs(observed[ind]))\n",
    "    print('one-tail threshold of the difference isc values is ',threshod, 'p < ',sig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
